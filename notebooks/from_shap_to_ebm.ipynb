{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# From SHAP to EBM\n",
    "This notebook demonstrates how to use SHAP and EBM to explain a model's predictions.\n",
    "We'll use the diamonds dataset from the GGplot2 R library to train an XGBoost model to predict diamond prices.\n",
    "\n",
    "Then, we'll use SHAP to explain the model's predictions and visualize the feature importance.\n",
    "Finally, we'll train an EBM model and compare the results with the XGBoost model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Setup\n",
    "Let's start by importing the necessary libraries and loading the data.\n",
    "\n",
    "If you're on Colab, you should install the required libraries by running the following cell.\n",
    "\n",
    "Moreover, you will need to upload the `diamonds.csv` file to the Colab environment and change the path to `diamonds.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade -q pandas plotly shap xgboost interpret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import interpret\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "import shap\n",
    "import xgboost\n",
    "from interpret.glassbox import ExplainableBoostingRegressor\n",
    "from plotly.graph_objs import Figure\n",
    "\n",
    "pio.templates.default = \"plotly_white\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    pd.read_csv(\"../data/diamonds.csv\", index_col=0)\n",
    "    .sample(5000, random_state=42)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "We first need to remove any missing values and filter out any outliers.\n",
    "Moreover, we'll convert the categorical columns to ordered categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df.x > 0) & (df.y > 0) & (df.z > 0) & (df.z < 30)]\n",
    "df[\"cut\"] = pd.Categorical(\n",
    "    df[\"cut\"],\n",
    "    categories=[\"Fair\", \"Good\", \"Very Good\", \"Premium\", \"Ideal\"],\n",
    "    ordered=True,\n",
    ")\n",
    "df[\"color\"] = pd.Categorical(\n",
    "    df[\"color\"], categories=[\"J\", \"I\", \"H\", \"G\", \"F\", \"E\", \"D\"], ordered=True\n",
    ")\n",
    "df[\"clarity\"] = pd.Categorical(\n",
    "    df[\"clarity\"],\n",
    "    categories=[\"I1\", \"SI2\", \"SI1\", \"VS2\", \"VS1\", \"VVS2\", \"VVS1\", \"IF\"],\n",
    "    ordered=True,\n",
    ")\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "We lost 4 sample, nothing to worry about."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Now we do have categorical columns with ordered categories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "We'll create some charts to visualize the data to understand the relationships between the features.\n",
    "We'll start with a scatter matrix for the numerical features and violin plots for the categorical features.\n",
    "Then, we'll merge the numerical and categorical features to see how they affect the target variable by means of scatter plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_matrix(\n",
    "    df, dimensions=[\"carat\", \"depth\", \"table\", \"price\", \"x\", \"y\", \"z\"]\n",
    ")\n",
    "fig.update_layout(autosize=False, width=1200, height=1200)\n",
    "fig.update_traces(marker=dict(size=3, opacity=0.5))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_violin_for_variable(df: pd.DataFrame, variable: str) -> Figure:\n",
    "    return px.violin(\n",
    "        df,\n",
    "        x=variable,\n",
    "        y=\"price\",\n",
    "        color=variable,\n",
    "        title=\"Price by Cut\",\n",
    "        category_orders={variable: df[variable].cat.categories.to_list()},\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_violin_for_variable(df, \"cut\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_violin_for_variable(df, \"color\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_violin_for_variable(df, \"clarity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(df, x=\"carat\", y=\"price\", color=\"cut\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "We can conclude that:\n",
    "- The price increases with the carat size.\n",
    "- The price is higher for diamonds with a better cut, color, and clarity.\n",
    "- Depth and table don't seem to have a significant impact on the price.\n",
    "- The dimensions x, y, and z are highly correlated with each other and the carat size."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "# Modelling with XGBoost\n",
    "We'll train an XGBoost model to predict the diamond prices.\n",
    "\n",
    "Feel free to change the set of features to see what happens. In particular, you can try removing the dimensions `y` and `z`, as we saw that they are highly correlated with `x`.\n",
    "\n",
    "Moreover, you may want to remove `depth`and `table`, as they don't seem to have a significant impact on the price.\n",
    "\n",
    "We do not care about hyperparameter tuning in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = df[\n",
    "    [\"carat\", \"cut\", \"color\", \"clarity\", \"depth\", \"table\", \"x\", \"y\", \"z\", \"price\"]\n",
    "].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = model_df.drop(columns=\"price\").sample(frac=0.8, random_state=42)\n",
    "test_x = model_df.drop(columns=\"price\").drop(train_x.index)\n",
    "train_y = model_df[\"price\"].loc[train_x.index]\n",
    "test_y = model_df[\"price\"].loc[test_x.index]\n",
    "train_x.shape, test_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgboost.XGBRegressor(\n",
    "    objective=\"reg:squarederror\",\n",
    "    max_depth=6,\n",
    "    eta=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    seed=42,\n",
    "    n_estimators=100,\n",
    "    enable_categorical=True,\n",
    ")\n",
    "model.fit(train_x, train_y)\n",
    "predicted_y = model.predict(test_x)\n",
    "prediction_df = pd.DataFrame({\"actual\": test_y, \"predicted\": predicted_y})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_gof(prediction_df: pd.DataFrame):\n",
    "    scatter_gof_fig = px.scatter(\n",
    "        prediction_df, x=\"predicted\", y=\"actual\", title=\"Goodness of Fit\"\n",
    "    )\n",
    "    scatter_gof_fig.add_shape(\n",
    "        type=\"line\",\n",
    "        x0=0,\n",
    "        y0=0,\n",
    "        x1=prediction_df[\"predicted\"].max(),\n",
    "        y1=prediction_df[\"predicted\"].max(),\n",
    "    )\n",
    "    scatter_gof_fig.update_layout(autosize=False, width=600, height=600)\n",
    "    scatter_gof_fig.show()\n",
    "    errors = prediction_df[\"actual\"] - prediction_df[\"predicted\"]\n",
    "    px.histogram(errors, title=\"Error Distribution\", nbins=500).update_layout(\n",
    "        showlegend=False\n",
    "    ).show()\n",
    "\n",
    "\n",
    "def compute_metrics(prediction_df: pd.DataFrame) -> dict[str, float]:\n",
    "    error = prediction_df[\"actual\"] - prediction_df[\"predicted\"]\n",
    "    mae = error.abs().mean()\n",
    "    rmse = (error**2).mean() ** 0.5\n",
    "    return {\"mae\": mae, \"rmse\": rmse}\n",
    "\n",
    "\n",
    "plot_gof(prediction_df)\n",
    "compute_metrics(prediction_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "The predictions are quite good, with most of the samples very close to the 45-degree line in the goodness of fit chart."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "As a first test, let's see the feature importance according to the XGBoost model.\n",
    "\n",
    "We must consider that this is an impurity-based metric, and it can suffer from several biases. In particular, it can be biased towards high cardinality features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = model.get_booster().get_score(importance_type=\"weight\")\n",
    "importance_df = pd.DataFrame(\n",
    "    {\"Feature\": list(importance.keys()), \"Importance\": list(importance.values())}\n",
    ")\n",
    "\n",
    "importance_df = importance_df.sort_values(by=\"Importance\", ascending=True)\n",
    "\n",
    "fig = px.bar(\n",
    "    importance_df,\n",
    "    x=\"Importance\",\n",
    "    y=\"Feature\",\n",
    "    orientation=\"h\",\n",
    "    title=\"Feature Importance\",\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "# SHAP\n",
    "Now, let's use SHAP to explain the model's predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer(train_x)\n",
    "shap.summary_plot(shap_values, train_x, plot_type=\"bar\", show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values, train_x, show=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_index = 0\n",
    "shap_values_test = explainer.shap_values(test_x)\n",
    "shap.force_plot(\n",
    "    explainer.expected_value,\n",
    "    shap_values_test[sample_index, :],\n",
    "    test_x.iloc[sample_index, :],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "We see that the output is quite similar to the feature importance plot we obtained from the XGBoost model, however not identical.\n",
    "\n",
    "Moreover, SHAP explains the direction of the effect of each feature on the prediction, which is very useful for understanding the model's behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "# EBM\n",
    "Finally, we'll train an EBM model to predict the diamond prices and compare the results with the XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm = ExplainableBoostingRegressor(random_state=42)\n",
    "ebm.fit(train_x, train_y)\n",
    "predicted_y_ebm = ebm.predict(test_x)\n",
    "prediction_df_ebm = pd.DataFrame({\"actual\": test_y, \"predicted\": predicted_y_ebm})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_gof(prediction_df_ebm)\n",
    "compute_metrics(prediction_df_ebm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "The EBM model performs slightly worse than the XGBoost model, with a higher MAE and RMSE. This is because the EBM model is constrained to be more interpretable, and it may not capture the underlying patterns as well as the XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm_global = ebm.explain_global()\n",
    "interpret.show(ebm_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "ebm_local = ebm.explain_local(test_x, test_y)\n",
    "interpret.show(ebm_local)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "On the other hand, the EBM model is more interpretable, and we can see the effect of each feature on the prediction for each sample. Differently from SHAP, such effects are not inferred a-posteriori with some game-theory approach, but directly estimated by the model parameters.\n",
    "\n",
    "This guarantees accurate explanations of both the global and local behavior of the model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
